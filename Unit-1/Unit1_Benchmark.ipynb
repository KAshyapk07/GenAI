{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6318f62e",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "Name : Kashyap K  \n",
    "SRN : PES2UG23CS263\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cc2943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "311fcd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is................................................................................................................................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "#EXP 1\n",
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"bert-base-uncased\")\n",
    "out = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(out[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc6781ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"roberta-base\")\n",
    "out = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(out[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8befa99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa25c054eaeb42fca15a8775a45abca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kashy\\.cache\\huggingface\\hub\\models--facebook--bart-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190f3a6ff63141b3afcace342f4c7daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557198eef5c345178ed47146279c2639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbf919a1f2a44af87dd41223f64642f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f822013f2d4f2483fd2aae40d84e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is islands islands islands Century judging dividing pit invited pan pan traffic compriserahrahrah perched traffic stereotype sample CenturyMED EPA phase accredited accredited freely applicable Feather fixes perched perched stereotype pessim173 Basin� Argentine phase applicable accredited accredited accreditedambquerade volatility entryophenophen Chemical stereotype accreditedamb violationsamb stereotype stereotype entryministic Shadow Shadow stereotype stereotype stereotype� stereotype� commercially filtering expects stereotype shielded filtering applicable applicable Feather filtering accredited shieldedquerade Kurdistan CHOquerade—\"ccoli violationsrawl happ shieldedworked stickerTroTroambINT fibre applicable happ Shadow shielded perchedccoliophen entryrawlquerade Richard413ophenrawlarus stereotype happvern shieldedquerade shieldedquerade Richard stickerministic restriction stickerophen stereotype shielded conclusions accreditedophen Shadowworkeditudes happ analogyophen Century accreditedNichvernvern expects abdomen abdomen stereotypevern dopingazon happ Fortressophen Emerging analogy happophen entry Herod Herodophen Herod stereotype aftermath expects drum Fifaquerade dopingvern dividingquerade sticker Fitquerade Windows bows pit stereotype happquerade FifaWire expects Partnerophenophenophen Partnerophenitudesquerade sticker conclusionsqueradeophen Herodvernvern`, expects abdomenceivable Genesis abdomen Emergingvern ferment abdomenitudesvern abdomen Genesis dowvern perched abdomenRAM abdomen abdomenvernvern abdomen abdomen abdomen Soccerophenitudes abdomen abdomen judgingitudes Reply dow abdomen abdomenTweitudes accredited Emergingophen abdomenanna abdomen abdomenophen Fifavern abdomen McK strives ferment dividing dividing abdomen abdomen manufacture abdomen stereotype\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"facebook/bart-base\")\n",
    "out = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(out[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5df840d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create -> 0.5396928787231445\n",
      "generate -> 0.15575754642486572\n",
      "produce -> 0.0540548674762249\n"
     ]
    }
   ],
   "source": [
    "#eperiment 2\n",
    "sentence = \"The goal of Generative AI is to [MASK] new content.\"\n",
    "\n",
    "fill = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "out = fill(sentence)\n",
    "for o in out[:3]:\n",
    "    print(o[\"token_str\"], \"->\", o[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bde4bca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.058836594223976135, 'start': 14, 'end': 81, 'answer': 'poses significant risks such as hallucinations, bias, and deepfakes'}\n"
     ]
    }
   ],
   "source": [
    "#eperiment 3\n",
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question = \"What are the risks?\"\n",
    "\n",
    "qa = pipeline(\"question-answering\", model=\"facebook/bart-base\")\n",
    "out = qa(question=question, context=context)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7608bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.004458255600184202, 'start': 20, 'end': 45, 'answer': 'significant risks such as'}\n"
     ]
    }
   ],
   "source": [
    "#eperiment 3\n",
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question = \"What are the risks?\"\n",
    "\n",
    "qa = pipeline(\"question-answering\", model=\"roberta-base\")\n",
    "out = qa(question=question, context=context)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac90c288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.014442062471061945, 'start': 66, 'end': 81, 'answer': ', and deepfakes'}\n"
     ]
    }
   ],
   "source": [
    "#eperiment 3\n",
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question = \"What are the risks?\"\n",
    "\n",
    "qa = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n",
    "out = qa(question=question, context=context)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc0e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
